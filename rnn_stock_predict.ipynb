{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "   # %tensorflow_version only exists in Colab.\n",
    "   %tensorflow_version 2.x\n",
    "except Exception:\n",
    " pass\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training set (1258, 6)\n",
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv') \n",
    "#index slice dataFrame : all row, column index 1=='open'(instead 1 -> 1:2) \n",
    "training_set = dataset_train.iloc[:, 1:2].values #Open colum data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Open    High     Low   Close      Volume\n",
       "0  1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1  1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2  1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3  1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4  1/9/2012  322.04  322.29  309.46  620.76  11,688,800"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset_train.shape)\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_set.shape)\n",
    "training_set #numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# sc = MinMaxScaler(feature_range = (0, 1))\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1197"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 1257-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 1257-1197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_set_scaled.shape)\n",
    "training_set_scaled # 2-d np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1258,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08581368, 0.09701243, 0.09433366, ..., 0.95725128, 0.93796041,\n",
       "       0.93688146])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(training_set_scaled[:,0].shape)\n",
    "i [:,0] #make 1-d np.array with 60 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* i :  60\n",
      "*training_set_scaled[0:60, 0] \n",
      "\n",
      "[0.08581368 0.09701243 0.09433366 0.09156187 0.07984225 0.0643277\n",
      " 0.0585423  0.06568569 0.06109085 0.06639259 0.0614257  0.07474514\n",
      " 0.02797827 0.02379269 0.02409033 0.0159238  0.01078949 0.00967334\n",
      " 0.01642607 0.02100231 0.02280676 0.02273235 0.02810849 0.03212665\n",
      " 0.0433812  0.04475779 0.04790163 0.0440695  0.04648783 0.04745517\n",
      " 0.04873875 0.03936305 0.04137213 0.04034898 0.04784582 0.04325099\n",
      " 0.04356723 0.04286033 0.04602277 0.05398467 0.05738894 0.05714711\n",
      " 0.05569611 0.04421832 0.04514845 0.04605997 0.04412531 0.03675869\n",
      " 0.04486941 0.05065481 0.05214302 0.05612397 0.05818885 0.06540665\n",
      " 0.06882953 0.07243843 0.07993526 0.07846566 0.08034452 0.08497656]\n",
      "*X_train1.append(training_set_scaled[0:60, 0])\n",
      "* X_train1 - number of appended list :  1\n",
      "----------------------------------------------------\n",
      "\n",
      "**training_set_scaled[60, 0]\n",
      "0.08627874097775134\n",
      "**y_train1.append(training_set_scaled[60, 0]) \n",
      "** y_train1 - number of appended list :  1\n",
      "================================================================\n",
      "* i :  61\n",
      "*training_set_scaled[1:61, 0] \n",
      "\n",
      "[0.09701243 0.09433366 0.09156187 0.07984225 0.0643277  0.0585423\n",
      " 0.06568569 0.06109085 0.06639259 0.0614257  0.07474514 0.02797827\n",
      " 0.02379269 0.02409033 0.0159238  0.01078949 0.00967334 0.01642607\n",
      " 0.02100231 0.02280676 0.02273235 0.02810849 0.03212665 0.0433812\n",
      " 0.04475779 0.04790163 0.0440695  0.04648783 0.04745517 0.04873875\n",
      " 0.03936305 0.04137213 0.04034898 0.04784582 0.04325099 0.04356723\n",
      " 0.04286033 0.04602277 0.05398467 0.05738894 0.05714711 0.05569611\n",
      " 0.04421832 0.04514845 0.04605997 0.04412531 0.03675869 0.04486941\n",
      " 0.05065481 0.05214302 0.05612397 0.05818885 0.06540665 0.06882953\n",
      " 0.07243843 0.07993526 0.07846566 0.08034452 0.08497656 0.08627874]\n",
      "*X_train1.append(training_set_scaled[1:61, 0])\n",
      "* X_train1 - number of appended list :  2\n",
      "----------------------------------------------------\n",
      "\n",
      "**training_set_scaled[61, 0]\n",
      "0.08471612471166012\n",
      "**y_train1.append(training_set_scaled[61, 0]) \n",
      "** y_train1 - number of appended list :  2\n",
      "================================================================\n",
      "* i :  62\n",
      "*training_set_scaled[2:62, 0] \n",
      "\n",
      "[0.09433366 0.09156187 0.07984225 0.0643277  0.0585423  0.06568569\n",
      " 0.06109085 0.06639259 0.0614257  0.07474514 0.02797827 0.02379269\n",
      " 0.02409033 0.0159238  0.01078949 0.00967334 0.01642607 0.02100231\n",
      " 0.02280676 0.02273235 0.02810849 0.03212665 0.0433812  0.04475779\n",
      " 0.04790163 0.0440695  0.04648783 0.04745517 0.04873875 0.03936305\n",
      " 0.04137213 0.04034898 0.04784582 0.04325099 0.04356723 0.04286033\n",
      " 0.04602277 0.05398467 0.05738894 0.05714711 0.05569611 0.04421832\n",
      " 0.04514845 0.04605997 0.04412531 0.03675869 0.04486941 0.05065481\n",
      " 0.05214302 0.05612397 0.05818885 0.06540665 0.06882953 0.07243843\n",
      " 0.07993526 0.07846566 0.08034452 0.08497656 0.08627874 0.08471612]\n",
      "*X_train1.append(training_set_scaled[2:62, 0])\n",
      "* X_train1 - number of appended list :  3\n",
      "----------------------------------------------------\n",
      "\n",
      "**training_set_scaled[62, 0]\n",
      "0.07454051640747084\n",
      "**y_train1.append(training_set_scaled[62, 0]) \n",
      "** y_train1 - number of appended list :  3\n",
      "================================================================\n",
      "* i :  63\n",
      "*training_set_scaled[3:63, 0] \n",
      "\n",
      "[0.09156187 0.07984225 0.0643277  0.0585423  0.06568569 0.06109085\n",
      " 0.06639259 0.0614257  0.07474514 0.02797827 0.02379269 0.02409033\n",
      " 0.0159238  0.01078949 0.00967334 0.01642607 0.02100231 0.02280676\n",
      " 0.02273235 0.02810849 0.03212665 0.0433812  0.04475779 0.04790163\n",
      " 0.0440695  0.04648783 0.04745517 0.04873875 0.03936305 0.04137213\n",
      " 0.04034898 0.04784582 0.04325099 0.04356723 0.04286033 0.04602277\n",
      " 0.05398467 0.05738894 0.05714711 0.05569611 0.04421832 0.04514845\n",
      " 0.04605997 0.04412531 0.03675869 0.04486941 0.05065481 0.05214302\n",
      " 0.05612397 0.05818885 0.06540665 0.06882953 0.07243843 0.07993526\n",
      " 0.07846566 0.08034452 0.08497656 0.08627874 0.08471612 0.07454052]\n",
      "*X_train1.append(training_set_scaled[3:63, 0])\n",
      "* X_train1 - number of appended list :  4\n",
      "----------------------------------------------------\n",
      "\n",
      "**training_set_scaled[63, 0]\n",
      "0.07883771113922167\n",
      "**y_train1.append(training_set_scaled[63, 0]) \n",
      "** y_train1 - number of appended list :  4\n",
      "================================================================\n",
      "* i :  64\n",
      "*training_set_scaled[4:64, 0] \n",
      "\n",
      "[0.07984225 0.0643277  0.0585423  0.06568569 0.06109085 0.06639259\n",
      " 0.0614257  0.07474514 0.02797827 0.02379269 0.02409033 0.0159238\n",
      " 0.01078949 0.00967334 0.01642607 0.02100231 0.02280676 0.02273235\n",
      " 0.02810849 0.03212665 0.0433812  0.04475779 0.04790163 0.0440695\n",
      " 0.04648783 0.04745517 0.04873875 0.03936305 0.04137213 0.04034898\n",
      " 0.04784582 0.04325099 0.04356723 0.04286033 0.04602277 0.05398467\n",
      " 0.05738894 0.05714711 0.05569611 0.04421832 0.04514845 0.04605997\n",
      " 0.04412531 0.03675869 0.04486941 0.05065481 0.05214302 0.05612397\n",
      " 0.05818885 0.06540665 0.06882953 0.07243843 0.07993526 0.07846566\n",
      " 0.08034452 0.08497656 0.08627874 0.08471612 0.07454052 0.07883771]\n",
      "*X_train1.append(training_set_scaled[4:64, 0])\n",
      "* X_train1 - number of appended list :  5\n",
      "----------------------------------------------------\n",
      "\n",
      "**training_set_scaled[64, 0]\n",
      "0.07238261775429711\n",
      "**y_train1.append(training_set_scaled[64, 0]) \n",
      "** y_train1 - number of appended list :  5\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test -> similar generator(from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator)\n",
    "# Creating a data structure('open' price) with 60 timesteps and 1 output\n",
    "# Same as generator(from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator)\n",
    "X_train1 = []\n",
    "y_train1 = []\n",
    "for i in range(60, 65): #from 60 to 65-1 \n",
    "    print(\"* i : \", i)\n",
    "    X_train1.append(training_set_scaled[i-60:i, 0])#if i == 60, [0:60, 0], 61 [1:61, 0], 62 [2:62, 0],,, 64 [4:64, 0]\n",
    "    print(f\"*training_set_scaled[{i-60}:{i}, 0] \\n\") #[0:60, 0] -> [0,0],,,[59,0] -> [60,0] not incl, which is y_train.\n",
    "    print(training_set_scaled[i-60:i, 0])\n",
    "    \n",
    "    print(f\"*X_train1.append(training_set_scaled[{i-60}:{i}, 0])\")\n",
    "    print(\"* X_train1 - number of appended list : \",len(X_train1)) #list\n",
    "  \n",
    "    print(\"----------------------------------------------------\\n\")\n",
    "    y_train1.append(training_set_scaled[i, 0]) #[60,0], [61,0], [62,0],,,[64,0]\n",
    "    print(f\"**training_set_scaled[{i}, 0]\")\n",
    "    print(training_set_scaled[i, 0])\n",
    "    \n",
    "    print(f\"**y_train1.append(training_set_scaled[{i}, 0]) \")\n",
    "    print(\"** y_train1 - number of appended list : \",len(y_train1)) #list\n",
    "    \n",
    "    print(\"================================================================\")\n",
    "X_train1, y_train1 = np.array(X_train1), np.array(y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, 0.09156187, 0.07984225,\n",
       "        0.0643277 , 0.0585423 , 0.06568569, 0.06109085, 0.06639259,\n",
       "        0.0614257 , 0.07474514, 0.02797827, 0.02379269, 0.02409033,\n",
       "        0.0159238 , 0.01078949, 0.00967334, 0.01642607, 0.02100231,\n",
       "        0.02280676, 0.02273235, 0.02810849, 0.03212665, 0.0433812 ,\n",
       "        0.04475779, 0.04790163, 0.0440695 , 0.04648783, 0.04745517,\n",
       "        0.04873875, 0.03936305, 0.04137213, 0.04034898, 0.04784582,\n",
       "        0.04325099, 0.04356723, 0.04286033, 0.04602277, 0.05398467,\n",
       "        0.05738894, 0.05714711, 0.05569611, 0.04421832, 0.04514845,\n",
       "        0.04605997, 0.04412531, 0.03675869, 0.04486941, 0.05065481,\n",
       "        0.05214302, 0.05612397, 0.05818885, 0.06540665, 0.06882953,\n",
       "        0.07243843, 0.07993526, 0.07846566, 0.08034452, 0.08497656],\n",
       "       [0.09701243, 0.09433366, 0.09156187, 0.07984225, 0.0643277 ,\n",
       "        0.0585423 , 0.06568569, 0.06109085, 0.06639259, 0.0614257 ,\n",
       "        0.07474514, 0.02797827, 0.02379269, 0.02409033, 0.0159238 ,\n",
       "        0.01078949, 0.00967334, 0.01642607, 0.02100231, 0.02280676,\n",
       "        0.02273235, 0.02810849, 0.03212665, 0.0433812 , 0.04475779,\n",
       "        0.04790163, 0.0440695 , 0.04648783, 0.04745517, 0.04873875,\n",
       "        0.03936305, 0.04137213, 0.04034898, 0.04784582, 0.04325099,\n",
       "        0.04356723, 0.04286033, 0.04602277, 0.05398467, 0.05738894,\n",
       "        0.05714711, 0.05569611, 0.04421832, 0.04514845, 0.04605997,\n",
       "        0.04412531, 0.03675869, 0.04486941, 0.05065481, 0.05214302,\n",
       "        0.05612397, 0.05818885, 0.06540665, 0.06882953, 0.07243843,\n",
       "        0.07993526, 0.07846566, 0.08034452, 0.08497656, 0.08627874],\n",
       "       [0.09433366, 0.09156187, 0.07984225, 0.0643277 , 0.0585423 ,\n",
       "        0.06568569, 0.06109085, 0.06639259, 0.0614257 , 0.07474514,\n",
       "        0.02797827, 0.02379269, 0.02409033, 0.0159238 , 0.01078949,\n",
       "        0.00967334, 0.01642607, 0.02100231, 0.02280676, 0.02273235,\n",
       "        0.02810849, 0.03212665, 0.0433812 , 0.04475779, 0.04790163,\n",
       "        0.0440695 , 0.04648783, 0.04745517, 0.04873875, 0.03936305,\n",
       "        0.04137213, 0.04034898, 0.04784582, 0.04325099, 0.04356723,\n",
       "        0.04286033, 0.04602277, 0.05398467, 0.05738894, 0.05714711,\n",
       "        0.05569611, 0.04421832, 0.04514845, 0.04605997, 0.04412531,\n",
       "        0.03675869, 0.04486941, 0.05065481, 0.05214302, 0.05612397,\n",
       "        0.05818885, 0.06540665, 0.06882953, 0.07243843, 0.07993526,\n",
       "        0.07846566, 0.08034452, 0.08497656, 0.08627874, 0.08471612],\n",
       "       [0.09156187, 0.07984225, 0.0643277 , 0.0585423 , 0.06568569,\n",
       "        0.06109085, 0.06639259, 0.0614257 , 0.07474514, 0.02797827,\n",
       "        0.02379269, 0.02409033, 0.0159238 , 0.01078949, 0.00967334,\n",
       "        0.01642607, 0.02100231, 0.02280676, 0.02273235, 0.02810849,\n",
       "        0.03212665, 0.0433812 , 0.04475779, 0.04790163, 0.0440695 ,\n",
       "        0.04648783, 0.04745517, 0.04873875, 0.03936305, 0.04137213,\n",
       "        0.04034898, 0.04784582, 0.04325099, 0.04356723, 0.04286033,\n",
       "        0.04602277, 0.05398467, 0.05738894, 0.05714711, 0.05569611,\n",
       "        0.04421832, 0.04514845, 0.04605997, 0.04412531, 0.03675869,\n",
       "        0.04486941, 0.05065481, 0.05214302, 0.05612397, 0.05818885,\n",
       "        0.06540665, 0.06882953, 0.07243843, 0.07993526, 0.07846566,\n",
       "        0.08034452, 0.08497656, 0.08627874, 0.08471612, 0.07454052],\n",
       "       [0.07984225, 0.0643277 , 0.0585423 , 0.06568569, 0.06109085,\n",
       "        0.06639259, 0.0614257 , 0.07474514, 0.02797827, 0.02379269,\n",
       "        0.02409033, 0.0159238 , 0.01078949, 0.00967334, 0.01642607,\n",
       "        0.02100231, 0.02280676, 0.02273235, 0.02810849, 0.03212665,\n",
       "        0.0433812 , 0.04475779, 0.04790163, 0.0440695 , 0.04648783,\n",
       "        0.04745517, 0.04873875, 0.03936305, 0.04137213, 0.04034898,\n",
       "        0.04784582, 0.04325099, 0.04356723, 0.04286033, 0.04602277,\n",
       "        0.05398467, 0.05738894, 0.05714711, 0.05569611, 0.04421832,\n",
       "        0.04514845, 0.04605997, 0.04412531, 0.03675869, 0.04486941,\n",
       "        0.05065481, 0.05214302, 0.05612397, 0.05818885, 0.06540665,\n",
       "        0.06882953, 0.07243843, 0.07993526, 0.07846566, 0.08034452,\n",
       "        0.08497656, 0.08627874, 0.08471612, 0.07454052, 0.07883771]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train1.shape) #np.array\n",
    "X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08627874, 0.08471612, 0.07454052, 0.07883771, 0.07238262])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train1.shape) #np.array\n",
    "y_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data structure('open' price) with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258): #from 60 to 1258-1 \n",
    "    X_train.append(training_set_scaled[i-60:i, 0])#if i == 60, [0:60, 0], 61 [1:61, 0], 62 [2:62, 0],,, 1257 [1197:1257, 0]\n",
    "    y_train.append(training_set_scaled[i, 0]) #[60,0], [61,0], [62,0],,,[1257,0]\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198, 60)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, ..., 0.07846566, 0.08034452,\n",
       "        0.08497656],\n",
       "       [0.09701243, 0.09433366, 0.09156187, ..., 0.08034452, 0.08497656,\n",
       "        0.08627874],\n",
       "       [0.09433366, 0.09156187, 0.07984225, ..., 0.08497656, 0.08627874,\n",
       "        0.08471612],\n",
       "       ...,\n",
       "       [0.92106928, 0.92438053, 0.93048218, ..., 0.95475854, 0.95204256,\n",
       "        0.95163331],\n",
       "       [0.92438053, 0.93048218, 0.9299055 , ..., 0.95204256, 0.95163331,\n",
       "        0.95725128],\n",
       "       [0.93048218, 0.9299055 , 0.93113327, ..., 0.95163331, 0.95725128,\n",
       "        0.93796041]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape) \n",
    "X_train # scaled 'open' price with 60 time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.08627874, 0.08471612, 0.07454052, ..., 0.95725128, 0.93796041,\n",
       "       0.93688146])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping:(1198, 60) -> (1198, 60, 1)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1)) #(1198,60,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 - Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import LSTM\n",
    "# from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1))) # 60,1\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1198 samples\n",
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0496\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 1s 701us/sample - loss: 0.0075\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 1s 697us/sample - loss: 0.0065\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 1s 712us/sample - loss: 0.0061\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 1s 715us/sample - loss: 0.0056\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 1s 692us/sample - loss: 0.0049\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 1s 709us/sample - loss: 0.0045\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 1s 706us/sample - loss: 0.0043\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 1s 705us/sample - loss: 0.0046\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 1s 700us/sample - loss: 0.0049\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 1s 697us/sample - loss: 0.0037\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 1s 683us/sample - loss: 0.0039\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 1s 709us/sample - loss: 0.0041\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 1s 711us/sample - loss: 0.0040\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 1s 684us/sample - loss: 0.0039\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 1s 677us/sample - loss: 0.0041\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 1s 707us/sample - loss: 0.0036\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 1s 702us/sample - loss: 0.0033\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 1s 715us/sample - loss: 0.0036\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 1s 700us/sample - loss: 0.0036\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 1s 692us/sample - loss: 0.0033\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 1s 693us/sample - loss: 0.0034\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 1s 689us/sample - loss: 0.0033\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 1s 692us/sample - loss: 0.0030\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 1s 630us/sample - loss: 0.0035\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 1s 582us/sample - loss: 0.0030\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 1s 593us/sample - loss: 0.0037\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 1s 689us/sample - loss: 0.0027\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 1s 699us/sample - loss: 0.0029\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 1s 682us/sample - loss: 0.0032\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 1s 685us/sample - loss: 0.0031\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 1s 683us/sample - loss: 0.0029\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 1s 696us/sample - loss: 0.0029\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 1s 675us/sample - loss: 0.0027\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 1s 661us/sample - loss: 0.0027\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 1s 638us/sample - loss: 0.0028\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 1s 635us/sample - loss: 0.0026\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 1s 664us/sample - loss: 0.0029\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 1s 698us/sample - loss: 0.0027\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 1s 691us/sample - loss: 0.0029\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 1s 650us/sample - loss: 0.0026\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 1s 692us/sample - loss: 0.0028\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 1s 709us/sample - loss: 0.0026\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 1s 713us/sample - loss: 0.0024\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 1s 670us/sample - loss: 0.0026\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 1s 710us/sample - loss: 0.0027\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 1s 669us/sample - loss: 0.0029\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 1s 675us/sample - loss: 0.0023\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 1s 706us/sample - loss: 0.0022\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 1s 665us/sample - loss: 0.0024\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 1s 633us/sample - loss: 0.0021\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 1s 697us/sample - loss: 0.0022\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 1s 695us/sample - loss: 0.0024\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 1s 686us/sample - loss: 0.0022\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 1s 699us/sample - loss: 0.0023\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 1s 676us/sample - loss: 0.0025\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 1s 700us/sample - loss: 0.0023\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 1s 707us/sample - loss: 0.0020\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 1s 700us/sample - loss: 0.0020\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 1s 711us/sample - loss: 0.0021\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 1s 703us/sample - loss: 0.0020\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 1s 696us/sample - loss: 0.0020\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 1s 691us/sample - loss: 0.0019\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 1s 681us/sample - loss: 0.0018\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 1s 697us/sample - loss: 0.0020\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 1s 702us/sample - loss: 0.0018\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 1s 703us/sample - loss: 0.0024\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 1s 700us/sample - loss: 0.0020\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 1s 709us/sample - loss: 0.0019\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 1s 678us/sample - loss: 0.0018\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 1s 686us/sample - loss: 0.0018\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 1s 695us/sample - loss: 0.0018\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 1s 694us/sample - loss: 0.0017\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 1s 713us/sample - loss: 0.0019\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 1s 700us/sample - loss: 0.0017\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 1s 693us/sample - loss: 0.0016\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 1s 707us/sample - loss: 0.0015\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 1s 691us/sample - loss: 0.0016\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 1s 715us/sample - loss: 0.0017\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 1s 706us/sample - loss: 0.0017\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 1s 708us/sample - loss: 0.0016\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 1s 698us/sample - loss: 0.0015\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 1s 711us/sample - loss: 0.0015\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 1s 718us/sample - loss: 0.0015\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 1s 677us/sample - loss: 0.0016\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 1s 702us/sample - loss: 0.0016\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 1s 716us/sample - loss: 0.0017\n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 1s 703us/sample - loss: 0.0015\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 1s 677us/sample - loss: 0.0015\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 1s 692us/sample - loss: 0.0014\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 1s 695us/sample - loss: 0.0015\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 1s 718us/sample - loss: 0.0014\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 1s 716us/sample - loss: 0.0015\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 1s 722us/sample - loss: 0.0016\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 1s 706us/sample - loss: 0.0017\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 1s 704us/sample - loss: 0.0014\n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 1s 695us/sample - loss: 0.0015\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 1s 699us/sample - loss: 0.0015\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 1s 665us/sample - loss: 0.0014\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 1s 719us/sample - loss: 0.0014\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f49e4056b90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>778.81</td>\n",
       "      <td>789.63</td>\n",
       "      <td>775.80</td>\n",
       "      <td>786.14</td>\n",
       "      <td>1,657,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2017</td>\n",
       "      <td>788.36</td>\n",
       "      <td>791.34</td>\n",
       "      <td>783.16</td>\n",
       "      <td>786.90</td>\n",
       "      <td>1,073,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2017</td>\n",
       "      <td>786.08</td>\n",
       "      <td>794.48</td>\n",
       "      <td>785.02</td>\n",
       "      <td>794.02</td>\n",
       "      <td>1,335,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2017</td>\n",
       "      <td>795.26</td>\n",
       "      <td>807.90</td>\n",
       "      <td>792.20</td>\n",
       "      <td>806.15</td>\n",
       "      <td>1,640,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2017</td>\n",
       "      <td>806.40</td>\n",
       "      <td>809.97</td>\n",
       "      <td>802.83</td>\n",
       "      <td>806.65</td>\n",
       "      <td>1,272,400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Open    High     Low   Close     Volume\n",
       "0  1/3/2017  778.81  789.63  775.80  786.14  1,657,300\n",
       "1  1/4/2017  788.36  791.34  783.16  786.90  1,073,000\n",
       "2  1/5/2017  786.08  794.48  785.02  794.02  1,335,200\n",
       "3  1/6/2017  795.26  807.90  792.20  806.15  1,640,200\n",
       "4  1/9/2017  806.40  809.97  802.83  806.65  1,272,400"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 3 - Making the predictions and visualising the results using Test csv file.(20, 6)\n",
    "\n",
    "# Getting the real stock price of 2017\n",
    "\n",
    "# dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "print(dataset_test.shape)\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[778.81],\n",
       "       [788.36],\n",
       "       [786.08],\n",
       "       [795.26],\n",
       "       [806.4 ],\n",
       "       [807.86],\n",
       "       [805.  ],\n",
       "       [807.14],\n",
       "       [807.48],\n",
       "       [807.08],\n",
       "       [805.81],\n",
       "       [805.12],\n",
       "       [806.91],\n",
       "       [807.25],\n",
       "       [822.3 ],\n",
       "       [829.62],\n",
       "       [837.81],\n",
       "       [834.71],\n",
       "       [814.66],\n",
       "       [796.86]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "print(real_stock_price.shape)\n",
    "real_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       325.25\n",
       "1       331.27\n",
       "2       329.83\n",
       "3       328.34\n",
       "4       322.04\n",
       "         ...  \n",
       "1253    790.90\n",
       "1254    790.68\n",
       "1255    793.70\n",
       "1256    783.33\n",
       "1257    782.75\n",
       "Name: Open, Length: 1258, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train['Open'] #1258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     778.81\n",
       "1     788.36\n",
       "2     786.08\n",
       "3     795.26\n",
       "4     806.40\n",
       "5     807.86\n",
       "6     805.00\n",
       "7     807.14\n",
       "8     807.48\n",
       "9     807.08\n",
       "10    805.81\n",
       "11    805.12\n",
       "12    806.91\n",
       "13    807.25\n",
       "14    822.30\n",
       "15    829.62\n",
       "16    837.81\n",
       "17    834.71\n",
       "18    814.66\n",
       "19    796.86\n",
       "Name: Open, dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test['Open'] #20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1278,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     325.25\n",
       "1     331.27\n",
       "2     329.83\n",
       "3     328.34\n",
       "4     322.04\n",
       "       ...  \n",
       "15    829.62\n",
       "16    837.81\n",
       "17    834.71\n",
       "18    814.66\n",
       "19    796.86\n",
       "Name: Open, Length: 1278, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the predicted stock price of 2017\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0) #1258 + 20 => 1278\n",
    "print(dataset_total.shape)\n",
    "dataset_total #1278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1198"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset_total))\n",
    "print(len(dataset_test))\n",
    "1278 - 20 - 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80,)\n",
      "80\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([779.  , 779.66, 777.71, 786.66, 783.76, 781.22, 781.65, 779.8 ,\n",
       "       787.85, 798.24, 803.3 , 795.  , 804.9 , 816.68, 806.34, 801.  ,\n",
       "       808.35, 795.47, 782.89, 778.2 , 767.25, 750.66, 774.5 , 783.4 ,\n",
       "       779.94, 791.17, 756.54, 755.6 , 746.97, 755.2 , 766.92, 771.37,\n",
       "       762.61, 772.63, 767.73, 764.26, 760.  , 771.53, 770.07, 757.44,\n",
       "       744.59, 757.71, 764.73, 761.  , 772.48, 780.  , 785.04, 793.9 ,\n",
       "       797.4 , 797.34, 800.4 , 790.22, 796.76, 795.84, 792.36, 790.9 ,\n",
       "       790.68, 793.7 , 783.33, 782.75, 778.81, 788.36, 786.08, 795.26,\n",
       "       806.4 , 807.86, 805.  , 807.14, 807.48, 807.08, 805.81, 805.12,\n",
       "       806.91, 807.25, 822.3 , 829.62, 837.81, 834.71, 814.66, 796.86])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[1278 - 20 - 60] =>  dataset_total[1198:].values\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "print(inputs.shape) #1-d\n",
    "print(len(inputs))\n",
    "inputs #dataset_total[1198:].values #80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 1)\n",
      "[[-0.5175052 ]\n",
      " [-0.51750291]\n",
      " [-0.51750966]\n",
      " [-0.51747869]\n",
      " [-0.51748873]\n",
      " [-0.51749752]\n",
      " [-0.51749603]\n",
      " [-0.51750243]\n",
      " [-0.51747457]\n",
      " [-0.51743862]\n",
      " [-0.51742111]\n",
      " [-0.51744983]\n",
      " [-0.51741557]\n",
      " [-0.5173748 ]\n",
      " [-0.51741059]\n",
      " [-0.51742907]\n",
      " [-0.51740363]\n",
      " [-0.5174482 ]\n",
      " [-0.51749174]\n",
      " [-0.51750797]\n",
      " [-0.51754586]\n",
      " [-0.51760327]\n",
      " [-0.51752077]\n",
      " [-0.51748997]\n",
      " [-0.51750195]\n",
      " [-0.51746308]\n",
      " [-0.51758292]\n",
      " [-0.51758618]\n",
      " [-0.51761604]\n",
      " [-0.51758756]\n",
      " [-0.517547  ]\n",
      " [-0.5175316 ]\n",
      " [-0.51756192]\n",
      " [-0.51752724]\n",
      " [-0.5175442 ]\n",
      " [-0.51755621]\n",
      " [-0.51757095]\n",
      " [-0.51753105]\n",
      " [-0.5175361 ]\n",
      " [-0.51757981]\n",
      " [-0.51762428]\n",
      " [-0.51757887]\n",
      " [-0.51755458]\n",
      " [-0.51756749]\n",
      " [-0.51752776]\n",
      " [-0.51750174]\n",
      " [-0.5174843 ]\n",
      " [-0.51745364]\n",
      " [-0.51744152]\n",
      " [-0.51744173]\n",
      " [-0.51743114]\n",
      " [-0.51746637]\n",
      " [-0.51744374]\n",
      " [-0.51744692]\n",
      " [-0.51745897]\n",
      " [-0.51746402]\n",
      " [-0.51746478]\n",
      " [-0.51745433]\n",
      " [-0.51749021]\n",
      " [-0.51749222]\n",
      " [-0.51750586]\n",
      " [-0.51747281]\n",
      " [-0.5174807 ]\n",
      " [-0.51744893]\n",
      " [-0.51741038]\n",
      " [-0.51740533]\n",
      " [-0.51741522]\n",
      " [-0.51740782]\n",
      " [-0.51740664]\n",
      " [-0.51740803]\n",
      " [-0.51741242]\n",
      " [-0.51741481]\n",
      " [-0.51740861]\n",
      " [-0.51740744]\n",
      " [-0.51735536]\n",
      " [-0.51733003]\n",
      " [-0.51730168]\n",
      " [-0.51731241]\n",
      " [-0.51738179]\n",
      " [-0.51744339]]\n",
      "-----------------\n",
      "(80, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.52019776],\n",
       "       [-0.52019775],\n",
       "       [-0.52019776],\n",
       "       [-0.52019771],\n",
       "       [-0.52019772],\n",
       "       [-0.52019774],\n",
       "       [-0.52019774],\n",
       "       [-0.52019775],\n",
       "       [-0.5201977 ],\n",
       "       [-0.52019763],\n",
       "       [-0.5201976 ],\n",
       "       [-0.52019765],\n",
       "       [-0.52019759],\n",
       "       [-0.52019751],\n",
       "       [-0.52019758],\n",
       "       [-0.52019761],\n",
       "       [-0.52019757],\n",
       "       [-0.52019765],\n",
       "       [-0.52019773],\n",
       "       [-0.52019776],\n",
       "       [-0.52019783],\n",
       "       [-0.52019794],\n",
       "       [-0.52019778],\n",
       "       [-0.52019773],\n",
       "       [-0.52019775],\n",
       "       [-0.52019768],\n",
       "       [-0.5201979 ],\n",
       "       [-0.52019791],\n",
       "       [-0.52019796],\n",
       "       [-0.52019791],\n",
       "       [-0.52019783],\n",
       "       [-0.5201978 ],\n",
       "       [-0.52019786],\n",
       "       [-0.5201978 ],\n",
       "       [-0.52019783],\n",
       "       [-0.52019785],\n",
       "       [-0.52019788],\n",
       "       [-0.5201978 ],\n",
       "       [-0.52019781],\n",
       "       [-0.52019789],\n",
       "       [-0.52019798],\n",
       "       [-0.52019789],\n",
       "       [-0.52019785],\n",
       "       [-0.52019787],\n",
       "       [-0.5201978 ],\n",
       "       [-0.52019775],\n",
       "       [-0.52019772],\n",
       "       [-0.52019766],\n",
       "       [-0.52019764],\n",
       "       [-0.52019764],\n",
       "       [-0.52019762],\n",
       "       [-0.52019768],\n",
       "       [-0.52019764],\n",
       "       [-0.52019765],\n",
       "       [-0.52019767],\n",
       "       [-0.52019768],\n",
       "       [-0.52019768],\n",
       "       [-0.52019766],\n",
       "       [-0.52019773],\n",
       "       [-0.52019773],\n",
       "       [-0.52019776],\n",
       "       [-0.52019769],\n",
       "       [-0.52019771],\n",
       "       [-0.52019765],\n",
       "       [-0.52019758],\n",
       "       [-0.52019757],\n",
       "       [-0.52019759],\n",
       "       [-0.52019757],\n",
       "       [-0.52019757],\n",
       "       [-0.52019757],\n",
       "       [-0.52019758],\n",
       "       [-0.52019759],\n",
       "       [-0.52019758],\n",
       "       [-0.52019757],\n",
       "       [-0.52019748],\n",
       "       [-0.52019743],\n",
       "       [-0.52019738],\n",
       "       [-0.5201974 ],\n",
       "       [-0.52019753],\n",
       "       [-0.52019764]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = inputs.reshape(-1,1)\n",
    "print(inputs.shape) #2-d\n",
    "print(inputs)\n",
    "print(\"-----------------\")\n",
    "inputs = sc.transform(inputs)\n",
    "print(inputs.shape)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* inputs[0:60, 0] \n",
      "\n",
      "[0.9299055  0.93113327 0.92750577 0.94415507 0.93876032 0.93403527\n",
      " 0.93483518 0.9313937  0.94636878 0.96569685 0.97510976 0.95966962\n",
      " 0.97808617 1.         0.98076494 0.97083116 0.98450406 0.96054394\n",
      " 0.9371419  0.92841729 0.90804747 0.8771858  0.92153434 0.93809063\n",
      " 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137\n",
      " 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853\n",
      " 0.89456061 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062\n",
      " 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334\n",
      " 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223\n",
      " 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146]\n",
      "** X_test.append(inputs[0:60, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[1:61, 0] \n",
      "\n",
      "[0.93113327 0.92750577 0.94415507 0.93876032 0.93403527 0.93483518\n",
      " 0.9313937  0.94636878 0.96569685 0.97510976 0.95966962 0.97808617\n",
      " 1.         0.98076494 0.97083116 0.98450406 0.96054394 0.9371419\n",
      " 0.92841729 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414\n",
      " 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359\n",
      " 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061\n",
      " 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962\n",
      " 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424\n",
      " 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854\n",
      " 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205]\n",
      "** X_test.append(inputs[1:61, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[2:62, 0] \n",
      "\n",
      "[0.92750577 0.94415507 0.93876032 0.93403527 0.93483518 0.9313937\n",
      " 0.94636878 0.96569685 0.97510976 0.95966962 0.97808617 1.\n",
      " 0.98076494 0.97083116 0.98450406 0.96054394 0.9371419  0.92841729\n",
      " 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414 0.95254483\n",
      " 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359 0.91571173\n",
      " 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061 0.91600938\n",
      " 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962 0.89642086\n",
      " 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424 0.96402262\n",
      " 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854 0.95204256\n",
      " 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205 0.94731751]\n",
      "** X_test.append(inputs[2:62, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[3:63, 0] \n",
      "\n",
      "[0.94415507 0.93876032 0.93403527 0.93483518 0.9313937  0.94636878\n",
      " 0.96569685 0.97510976 0.95966962 0.97808617 1.         0.98076494\n",
      " 0.97083116 0.98450406 0.96054394 0.9371419  0.92841729 0.90804747\n",
      " 0.8771858  0.92153434 0.93809063 0.93165414 0.95254483 0.88812412\n",
      " 0.88637547 0.87032145 0.88563137 0.90743359 0.91571173 0.89941588\n",
      " 0.91805566 0.9089404  0.9024853  0.89456061 0.91600938 0.9132934\n",
      " 0.88979835 0.86589404 0.89030062 0.90335962 0.89642086 0.91777662\n",
      " 0.93176576 0.94114145 0.95762334 0.96413424 0.96402262 0.96971501\n",
      " 0.95077759 0.96294367 0.96123223 0.95475854 0.95204256 0.95163331\n",
      " 0.95725128 0.93796041 0.93688146 0.92955205 0.94731751 0.94307612]\n",
      "** X_test.append(inputs[3:63, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[4:64, 0] \n",
      "\n",
      "[0.93876032 0.93403527 0.93483518 0.9313937  0.94636878 0.96569685\n",
      " 0.97510976 0.95966962 0.97808617 1.         0.98076494 0.97083116\n",
      " 0.98450406 0.96054394 0.9371419  0.92841729 0.90804747 0.8771858\n",
      " 0.92153434 0.93809063 0.93165414 0.95254483 0.88812412 0.88637547\n",
      " 0.87032145 0.88563137 0.90743359 0.91571173 0.89941588 0.91805566\n",
      " 0.9089404  0.9024853  0.89456061 0.91600938 0.9132934  0.88979835\n",
      " 0.86589404 0.89030062 0.90335962 0.89642086 0.91777662 0.93176576\n",
      " 0.94114145 0.95762334 0.96413424 0.96402262 0.96971501 0.95077759\n",
      " 0.96294367 0.96123223 0.95475854 0.95204256 0.95163331 0.95725128\n",
      " 0.93796041 0.93688146 0.92955205 0.94731751 0.94307612 0.96015329]\n",
      "** X_test.append(inputs[4:64, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[5:65, 0] \n",
      "\n",
      "[0.93403527 0.93483518 0.9313937  0.94636878 0.96569685 0.97510976\n",
      " 0.95966962 0.97808617 1.         0.98076494 0.97083116 0.98450406\n",
      " 0.96054394 0.9371419  0.92841729 0.90804747 0.8771858  0.92153434\n",
      " 0.93809063 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145\n",
      " 0.88563137 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404\n",
      " 0.9024853  0.89456061 0.91600938 0.9132934  0.88979835 0.86589404\n",
      " 0.89030062 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145\n",
      " 0.95762334 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367\n",
      " 0.96123223 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041\n",
      " 0.93688146 0.92955205 0.94731751 0.94307612 0.96015329 0.98087655]\n",
      "** X_test.append(inputs[5:65, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[6:66, 0] \n",
      "\n",
      "[0.93483518 0.9313937  0.94636878 0.96569685 0.97510976 0.95966962\n",
      " 0.97808617 1.         0.98076494 0.97083116 0.98450406 0.96054394\n",
      " 0.9371419  0.92841729 0.90804747 0.8771858  0.92153434 0.93809063\n",
      " 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137\n",
      " 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853\n",
      " 0.89456061 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062\n",
      " 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334\n",
      " 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223\n",
      " 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146\n",
      " 0.92955205 0.94731751 0.94307612 0.96015329 0.98087655 0.98359253]\n",
      "** X_test.append(inputs[6:66, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[7:67, 0] \n",
      "\n",
      "[0.9313937  0.94636878 0.96569685 0.97510976 0.95966962 0.97808617\n",
      " 1.         0.98076494 0.97083116 0.98450406 0.96054394 0.9371419\n",
      " 0.92841729 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414\n",
      " 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359\n",
      " 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061\n",
      " 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962\n",
      " 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424\n",
      " 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854\n",
      " 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205\n",
      " 0.94731751 0.94307612 0.96015329 0.98087655 0.98359253 0.97827219]\n",
      "** X_test.append(inputs[7:67, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[8:68, 0] \n",
      "\n",
      "[0.94636878 0.96569685 0.97510976 0.95966962 0.97808617 1.\n",
      " 0.98076494 0.97083116 0.98450406 0.96054394 0.9371419  0.92841729\n",
      " 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414 0.95254483\n",
      " 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359 0.91571173\n",
      " 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061 0.91600938\n",
      " 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962 0.89642086\n",
      " 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424 0.96402262\n",
      " 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854 0.95204256\n",
      " 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205 0.94731751\n",
      " 0.94307612 0.96015329 0.98087655 0.98359253 0.97827219 0.98225314]\n",
      "** X_test.append(inputs[8:68, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[9:69, 0] \n",
      "\n",
      "[0.96569685 0.97510976 0.95966962 0.97808617 1.         0.98076494\n",
      " 0.97083116 0.98450406 0.96054394 0.9371419  0.92841729 0.90804747\n",
      " 0.8771858  0.92153434 0.93809063 0.93165414 0.95254483 0.88812412\n",
      " 0.88637547 0.87032145 0.88563137 0.90743359 0.91571173 0.89941588\n",
      " 0.91805566 0.9089404  0.9024853  0.89456061 0.91600938 0.9132934\n",
      " 0.88979835 0.86589404 0.89030062 0.90335962 0.89642086 0.91777662\n",
      " 0.93176576 0.94114145 0.95762334 0.96413424 0.96402262 0.96971501\n",
      " 0.95077759 0.96294367 0.96123223 0.95475854 0.95204256 0.95163331\n",
      " 0.95725128 0.93796041 0.93688146 0.92955205 0.94731751 0.94307612\n",
      " 0.96015329 0.98087655 0.98359253 0.97827219 0.98225314 0.98288563]\n",
      "** X_test.append(inputs[9:69, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[10:70, 0] \n",
      "\n",
      "[0.97510976 0.95966962 0.97808617 1.         0.98076494 0.97083116\n",
      " 0.98450406 0.96054394 0.9371419  0.92841729 0.90804747 0.8771858\n",
      " 0.92153434 0.93809063 0.93165414 0.95254483 0.88812412 0.88637547\n",
      " 0.87032145 0.88563137 0.90743359 0.91571173 0.89941588 0.91805566\n",
      " 0.9089404  0.9024853  0.89456061 0.91600938 0.9132934  0.88979835\n",
      " 0.86589404 0.89030062 0.90335962 0.89642086 0.91777662 0.93176576\n",
      " 0.94114145 0.95762334 0.96413424 0.96402262 0.96971501 0.95077759\n",
      " 0.96294367 0.96123223 0.95475854 0.95204256 0.95163331 0.95725128\n",
      " 0.93796041 0.93688146 0.92955205 0.94731751 0.94307612 0.96015329\n",
      " 0.98087655 0.98359253 0.97827219 0.98225314 0.98288563 0.98214153]\n",
      "** X_test.append(inputs[10:70, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[11:71, 0] \n",
      "\n",
      "[0.95966962 0.97808617 1.         0.98076494 0.97083116 0.98450406\n",
      " 0.96054394 0.9371419  0.92841729 0.90804747 0.8771858  0.92153434\n",
      " 0.93809063 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145\n",
      " 0.88563137 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404\n",
      " 0.9024853  0.89456061 0.91600938 0.9132934  0.88979835 0.86589404\n",
      " 0.89030062 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145\n",
      " 0.95762334 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367\n",
      " 0.96123223 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041\n",
      " 0.93688146 0.92955205 0.94731751 0.94307612 0.96015329 0.98087655\n",
      " 0.98359253 0.97827219 0.98225314 0.98288563 0.98214153 0.979779  ]\n",
      "** X_test.append(inputs[11:71, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[12:72, 0] \n",
      "\n",
      "[0.97808617 1.         0.98076494 0.97083116 0.98450406 0.96054394\n",
      " 0.9371419  0.92841729 0.90804747 0.8771858  0.92153434 0.93809063\n",
      " 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137\n",
      " 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853\n",
      " 0.89456061 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062\n",
      " 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334\n",
      " 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223\n",
      " 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146\n",
      " 0.92955205 0.94731751 0.94307612 0.96015329 0.98087655 0.98359253\n",
      " 0.97827219 0.98225314 0.98288563 0.98214153 0.979779   0.97849542]\n",
      "** X_test.append(inputs[12:72, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[13:73, 0] \n",
      "\n",
      "[1.         0.98076494 0.97083116 0.98450406 0.96054394 0.9371419\n",
      " 0.92841729 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414\n",
      " 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359\n",
      " 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061\n",
      " 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962\n",
      " 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424\n",
      " 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854\n",
      " 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205\n",
      " 0.94731751 0.94307612 0.96015329 0.98087655 0.98359253 0.97827219\n",
      " 0.98225314 0.98288563 0.98214153 0.979779   0.97849542 0.98182528]\n",
      "** X_test.append(inputs[13:73, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[14:74, 0] \n",
      "\n",
      "[0.98076494 0.97083116 0.98450406 0.96054394 0.9371419  0.92841729\n",
      " 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414 0.95254483\n",
      " 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359 0.91571173\n",
      " 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061 0.91600938\n",
      " 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962 0.89642086\n",
      " 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424 0.96402262\n",
      " 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854 0.95204256\n",
      " 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205 0.94731751\n",
      " 0.94307612 0.96015329 0.98087655 0.98359253 0.97827219 0.98225314\n",
      " 0.98288563 0.98214153 0.979779   0.97849542 0.98182528 0.98245777]\n",
      "** X_test.append(inputs[14:74, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[15:75, 0] \n",
      "\n",
      "[0.97083116 0.98450406 0.96054394 0.9371419  0.92841729 0.90804747\n",
      " 0.8771858  0.92153434 0.93809063 0.93165414 0.95254483 0.88812412\n",
      " 0.88637547 0.87032145 0.88563137 0.90743359 0.91571173 0.89941588\n",
      " 0.91805566 0.9089404  0.9024853  0.89456061 0.91600938 0.9132934\n",
      " 0.88979835 0.86589404 0.89030062 0.90335962 0.89642086 0.91777662\n",
      " 0.93176576 0.94114145 0.95762334 0.96413424 0.96402262 0.96971501\n",
      " 0.95077759 0.96294367 0.96123223 0.95475854 0.95204256 0.95163331\n",
      " 0.95725128 0.93796041 0.93688146 0.92955205 0.94731751 0.94307612\n",
      " 0.96015329 0.98087655 0.98359253 0.97827219 0.98225314 0.98288563\n",
      " 0.98214153 0.979779   0.97849542 0.98182528 0.98245777 1.01045465]\n",
      "** X_test.append(inputs[15:75, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[16:76, 0] \n",
      "\n",
      "[0.98450406 0.96054394 0.9371419  0.92841729 0.90804747 0.8771858\n",
      " 0.92153434 0.93809063 0.93165414 0.95254483 0.88812412 0.88637547\n",
      " 0.87032145 0.88563137 0.90743359 0.91571173 0.89941588 0.91805566\n",
      " 0.9089404  0.9024853  0.89456061 0.91600938 0.9132934  0.88979835\n",
      " 0.86589404 0.89030062 0.90335962 0.89642086 0.91777662 0.93176576\n",
      " 0.94114145 0.95762334 0.96413424 0.96402262 0.96971501 0.95077759\n",
      " 0.96294367 0.96123223 0.95475854 0.95204256 0.95163331 0.95725128\n",
      " 0.93796041 0.93688146 0.92955205 0.94731751 0.94307612 0.96015329\n",
      " 0.98087655 0.98359253 0.97827219 0.98225314 0.98288563 0.98214153\n",
      " 0.979779   0.97849542 0.98182528 0.98245777 1.01045465 1.02407173]\n",
      "** X_test.append(inputs[16:76, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[17:77, 0] \n",
      "\n",
      "[0.96054394 0.9371419  0.92841729 0.90804747 0.8771858  0.92153434\n",
      " 0.93809063 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145\n",
      " 0.88563137 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404\n",
      " 0.9024853  0.89456061 0.91600938 0.9132934  0.88979835 0.86589404\n",
      " 0.89030062 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145\n",
      " 0.95762334 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367\n",
      " 0.96123223 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041\n",
      " 0.93688146 0.92955205 0.94731751 0.94307612 0.96015329 0.98087655\n",
      " 0.98359253 0.97827219 0.98225314 0.98288563 0.98214153 0.979779\n",
      " 0.97849542 0.98182528 0.98245777 1.01045465 1.02407173 1.03930724]\n",
      "** X_test.append(inputs[17:77, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[18:78, 0] \n",
      "\n",
      "[0.9371419  0.92841729 0.90804747 0.8771858  0.92153434 0.93809063\n",
      " 0.93165414 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137\n",
      " 0.90743359 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853\n",
      " 0.89456061 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062\n",
      " 0.90335962 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334\n",
      " 0.96413424 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223\n",
      " 0.95475854 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146\n",
      " 0.92955205 0.94731751 0.94307612 0.96015329 0.98087655 0.98359253\n",
      " 0.97827219 0.98225314 0.98288563 0.98214153 0.979779   0.97849542\n",
      " 0.98182528 0.98245777 1.01045465 1.02407173 1.03930724 1.03354044]\n",
      "** X_test.append(inputs[18:78, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n",
      "* inputs[19:79, 0] \n",
      "\n",
      "[0.92841729 0.90804747 0.8771858  0.92153434 0.93809063 0.93165414\n",
      " 0.95254483 0.88812412 0.88637547 0.87032145 0.88563137 0.90743359\n",
      " 0.91571173 0.89941588 0.91805566 0.9089404  0.9024853  0.89456061\n",
      " 0.91600938 0.9132934  0.88979835 0.86589404 0.89030062 0.90335962\n",
      " 0.89642086 0.91777662 0.93176576 0.94114145 0.95762334 0.96413424\n",
      " 0.96402262 0.96971501 0.95077759 0.96294367 0.96123223 0.95475854\n",
      " 0.95204256 0.95163331 0.95725128 0.93796041 0.93688146 0.92955205\n",
      " 0.94731751 0.94307612 0.96015329 0.98087655 0.98359253 0.97827219\n",
      " 0.98225314 0.98288563 0.98214153 0.979779   0.97849542 0.98182528\n",
      " 0.98245777 1.01045465 1.02407173 1.03930724 1.03354044 0.99624228]\n",
      "** X_test.append(inputs[19:79, 0]) \n",
      "\n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    print(f\"* inputs[{i-60}:{i}, 0] \\n\")\n",
    "    print(inputs[i-60:i, 0])\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "    print(f\"** X_test.append(inputs[{i-60}:{i}, 0]) \\n\")\n",
    "    print(\"--------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 60)\n",
      "[[0.9299055  0.93113327 0.92750577 ... 0.95725128 0.93796041 0.93688146]\n",
      " [0.93113327 0.92750577 0.94415507 ... 0.93796041 0.93688146 0.92955205]\n",
      " [0.92750577 0.94415507 0.93876032 ... 0.93688146 0.92955205 0.94731751]\n",
      " ...\n",
      " [0.96054394 0.9371419  0.92841729 ... 1.01045465 1.02407173 1.03930724]\n",
      " [0.9371419  0.92841729 0.90804747 ... 1.02407173 1.03930724 1.03354044]\n",
      " [0.92841729 0.90804747 0.8771858  ... 1.03930724 1.03354044 0.99624228]]\n",
      "----------------------------------------------------\n",
      "(20, 60, 1)\n",
      "[[[0.9299055 ]\n",
      "  [0.93113327]\n",
      "  [0.92750577]\n",
      "  ...\n",
      "  [0.95725128]\n",
      "  [0.93796041]\n",
      "  [0.93688146]]\n",
      "\n",
      " [[0.93113327]\n",
      "  [0.92750577]\n",
      "  [0.94415507]\n",
      "  ...\n",
      "  [0.93796041]\n",
      "  [0.93688146]\n",
      "  [0.92955205]]\n",
      "\n",
      " [[0.92750577]\n",
      "  [0.94415507]\n",
      "  [0.93876032]\n",
      "  ...\n",
      "  [0.93688146]\n",
      "  [0.92955205]\n",
      "  [0.94731751]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.96054394]\n",
      "  [0.9371419 ]\n",
      "  [0.92841729]\n",
      "  ...\n",
      "  [1.01045465]\n",
      "  [1.02407173]\n",
      "  [1.03930724]]\n",
      "\n",
      " [[0.9371419 ]\n",
      "  [0.92841729]\n",
      "  [0.90804747]\n",
      "  ...\n",
      "  [1.02407173]\n",
      "  [1.03930724]\n",
      "  [1.03354044]]\n",
      "\n",
      " [[0.92841729]\n",
      "  [0.90804747]\n",
      "  [0.8771858 ]\n",
      "  ...\n",
      "  [1.03930724]\n",
      "  [1.03354044]\n",
      "  [0.99624228]]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array(X_test)\n",
    "print(X_test.shape) #(20, 60)\n",
    "print(X_test)\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "print(X_test.shape) #(20, 60, 1)\n",
    "print(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9208572 ],\n",
       "       [0.9158673 ],\n",
       "       [0.9174957 ],\n",
       "       [0.9200212 ],\n",
       "       [0.92608654],\n",
       "       [0.93668985],\n",
       "       [0.9449756 ],\n",
       "       [0.94684994],\n",
       "       [0.94669616],\n",
       "       [0.9464377 ],\n",
       "       [0.94650805],\n",
       "       [0.94650614],\n",
       "       [0.9466138 ],\n",
       "       [0.94812644],\n",
       "       [0.9500928 ],\n",
       "       [0.95966244],\n",
       "       [0.9721341 ],\n",
       "       [0.98440707],\n",
       "       [0.98924065],\n",
       "       [0.9787067 ]], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price = regressor.predict(X_test)#(20, 60, 1) -> (20, 1)\n",
    "print(predicted_stock_price.shape)\n",
    "predicted_stock_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[774.136  ],\n",
       "       [771.4536 ],\n",
       "       [772.329  ],\n",
       "       [773.6866 ],\n",
       "       [776.9471 ],\n",
       "       [782.647  ],\n",
       "       [787.1011 ],\n",
       "       [788.10864],\n",
       "       [788.02594],\n",
       "       [787.887  ],\n",
       "       [787.92487],\n",
       "       [787.9238 ],\n",
       "       [787.9817 ],\n",
       "       [788.7948 ],\n",
       "       [789.85187],\n",
       "       [794.9961 ],\n",
       "       [801.7004 ],\n",
       "       [808.29785],\n",
       "       [810.8962 ],\n",
       "       [805.2336 ]], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "print(predicted_stock_price.shape)\n",
    "predicted_stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference:  LSTM model \n",
    "```\n",
    "test_size = 24\n",
    "length  = 18\n",
    "n_features = 1\n",
    "train = df[:-test_size]\n",
    "test = df[-test_size:]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train)\n",
    "scaled_train = scaler.transform(train)\n",
    "scaled_test = scaler.transform(test)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "generator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=batch_size)\n",
    "```\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(LSTM(100,input_shape=(length, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "```\n",
    "\n",
    "```\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=3) \n",
    "validation_generator = TimeseriesGenerator(scaled_test,scaled_test, length=length, batch_size=1)\n",
    "model.fit_generator(generator, epochs = 20,\n",
    "                   validation_data=validation_generator,\n",
    "                   callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "```\n",
    "test_predictions =[]\n",
    "current_batch = scaled_train[-length:].reshape(1,length,n_features) \n",
    "\n",
    "for i in range(len(test)):\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    test_predictions.append(current_pred)\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "true_predictions = scaler.inverse_transform(test_predictions)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to use Keras TimeseriesGenerator for time series data](https://www.dlology.com/blog/how-to-use-keras-timeseriesgenerator-for-time-series-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the results\n",
    "\n",
    "#dataset_test -> real_stock_price -> (20, 1)\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price') \n",
    "## predicted_stock_price = regressor.predict(X_test)#(20, 60, 1) -> (20, 1)\n",
    "#predicted_stock_price -> (20, 1)\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
